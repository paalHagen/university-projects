{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obligatory assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Open the URL + read the HTML, create a bs4 object to parse the HTML (e.g convert\n",
    "# to a format that can be navigated and manipulated)\n",
    "html = urlopen(\"https://en.wikipedia.org/wiki/Star_Wars:_The_Rise_of_Skywalker\")\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Finds all a tags with an href attribute\n",
    "for link in bs.find_all(\"a\", href=True):\n",
    "    # We find the absolute path/URL by combining the base web_page and the\n",
    "    # relative path (rel_path) in the urljoin() function\n",
    "    web_page = \"https://en.wikipedia.org\"\n",
    "    rel_path = link.attrs[\"href\"]\n",
    "    abs_path = urljoin(web_page, rel_path)\n",
    "    print(abs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds all img tags in the bs4 object from task 1.1\n",
    "for img in bs.find_all(\"img\"):\n",
    "    # We find the absolute path/URL by combining the base web_page and the\n",
    "    # relative path (rel_path) in the urljoin() function\n",
    "    rel_path = img[\"src\"]\n",
    "    abs_path = urljoin(web_page, rel_path)\n",
    "    print(abs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code finds all td elements with a class = “yes table-yes2 notheme” (e.g awards won), and prints the text of their parent elements (tr elements). “Award” (th element) and “Date of Ceremony” (td element) are not printed out for every award that is won. This is because the parent we print out the text for does not necessarily contain “Award” and “Date of Ceremony”. “Golden Trailer Awards” and “Saturn Awards” are examples of this. In this case a new solution would be necessary, for instance printing out those categories separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE CODE FROM THE ASSIGNMENT TEXT (not written by Pål Hagen Størksen)\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Open the URL + read the HTML, create a bs4 object to parse the HTML (e.g convert\n",
    "# to a format that can be navigated and manipulated)\n",
    "html = urlopen(\"https://en.wikipedia.org/wiki/Star_Wars:_The_Rise_of_Skywalker\")\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Finds all td elements with a class =\"yes table-yes2 notheme\"\n",
    "tds = bs.find_all(\"td\", {\"class\": \"yes table-yes2 notheme\"})\n",
    "\n",
    "# Iterates through the td elements, and prints the text of its parents elements\n",
    "# (e.g all children of tr)\n",
    "for td in tds:\n",
    "    print(td.parent.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Function to get all links from the \"See also\" section\n",
    "def extracting_links():\n",
    "    # Open the URL and read the HTML\n",
    "    try:\n",
    "        html_ = urlopen(\"https://en.wikipedia.org/wiki/Web_scraping\")\n",
    "    # Error handling\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTP error: {e}\")\n",
    "        return None\n",
    "    # If the URL is opened, then a bs4 object is created to parse the HTML (e.g convert\n",
    "    # to a format that can be navigated and manipulated)\n",
    "    soup = BeautifulSoup(html_, \"html.parser\")\n",
    "    links = []\n",
    "    # Using .find and() .find_all() we get the li elements, and iterate over those\n",
    "    # to extract the href (relative links), which are appended to the links list\n",
    "    try:\n",
    "        li = soup.find(\"div\", class_=\"div-col\").find(\"ul\").find_all(\"li\")\n",
    "        for a in li:\n",
    "            a = a.find(\"a\")\n",
    "            links.append(a[\"href\"])\n",
    "    # Error handling\n",
    "    except AttributeError as e:\n",
    "        print(f\"Attribute error: {e}\")\n",
    "    # The function returns a list of relative links from the \"See Also\" section\n",
    "    return links\n",
    "\n",
    "# Function to open and print the first paragraph (of the links we got from the previous function)\n",
    "def open_and_print_from_selected_links(links):\n",
    "    # Set to keep track of which links have been visited (in case we come across a link twice)\n",
    "    visited = set()\n",
    "    # Iterate over the links, and make the absolute path using urljoin (like we have done earlier)\n",
    "    for rel_path in links:\n",
    "        web_page = \"https://en.wikipedia.org\"\n",
    "        abs_path = urljoin(web_page, rel_path)\n",
    "\n",
    "        # Check if the absolute path has previously been visited, if yes then we skip that link,\n",
    "        # if no then add it to the visited set\n",
    "        if abs_path in visited:\n",
    "            continue\n",
    "        visited.add(abs_path)\n",
    "        \n",
    "        # Open the URL, read the HTML, create a bs4 object to parse the HTML,\n",
    "        # find the first p html element, and if its there we get the text and remove whitespace\n",
    "        # at the start and end. We then check if there is anything inside the p, and the result is\n",
    "        # all the first p elements with actual strings. The reason for removing whitespace\n",
    "        # is because some p elements appear empty in some of the links.\n",
    "        try:\n",
    "            crawl = urlopen(abs_path)\n",
    "            bs = BeautifulSoup(crawl, \"html.parser\")\n",
    "            first_paragraph = bs.find(\"p\")\n",
    "            if first_paragraph:\n",
    "                p = first_paragraph.get_text().strip()\n",
    "                if p:\n",
    "                    print(f\"First paragraph of {abs_path}:\\n{p}\\n\")\n",
    "                # else: # remove comment to also see when empty strings appear in the first_paragraph\n",
    "                    # print(f\"This is an empty string (no text): {abs_path}\\n\")\n",
    "        # Error handling\n",
    "        except AttributeError as e:\n",
    "            print(f\"Attribute error: {e}\")\n",
    "        except HTTPError as e:\n",
    "            print(f\"HTTP error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Exception error: {e}\")\n",
    "\n",
    "# Function calls, and print of the links\n",
    "links = extracting_links()\n",
    "print(f\"All extracted links from the first function:\")\n",
    "for link in links:\n",
    "    print(link)\n",
    "print(\"\\n\")\n",
    "\n",
    "open_and_print_from_selected_links(links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
